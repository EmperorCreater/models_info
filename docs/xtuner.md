# XTuner: 大模型微调工具库

## 简介
XTuner 是由上海人工智能实验室开发的一个高效、灵活、全能的轻量化大模型微调工具库。它专注于大语言模型（LLM）和多模态图文模型（VLM）的预训练及轻量级微调，旨在降低大模型微调的门槛和成本。

## 核心特性

- **轻量化微调**：在有限的硬件资源下，如8GB显存，也能微调高达7B参数的模型。
- **多节点支持**：支持多节点跨设备微调更大尺度的模型（70B+）。
- **高性能算子**：自动分发高性能算子，如 FlashAttention、Triton kernels 等，以加速训练吞吐。
- **DeepSpeed 兼容**：轻松应用各种 ZeRO 训练优化策略，提高训练效率。
- **模型兼容性**：支持多种大语言模型和多模态图文模型，如 InternLM、Mixtral-8x7B、Llama 2、ChatGLM、Qwen、Baichuan 以及 LLaVA。

## 快速开始

以下是使用 XTuner 进行模型微调的基本步骤：

1. **加载模型**：选择并加载预训练模型。
2. **准备数据**：加载并预处理训练数据。
3. **配置训练**：设置训练参数，如学习率、批次大小等。
4. **训练模型**：启动训练过程。
5. **评估和测试**：评估模型性能并进行测试。

## 文档和资源

- **GitHub 仓库**：[InternLM/xtuner](https://github.com/InternLM/xtuner) - 源代码和项目文档。